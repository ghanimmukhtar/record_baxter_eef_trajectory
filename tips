How to use the Baxter
=====================

Setup commands

kinect

####  launch kinect:

roslaunch freenect_launch freenect.launch rgb_frame_id:=camera_rgb_optical_frame depth_frame_id:=camera_depth_optical_frame


((((((
if camera not calibrated:

launch calibration

roslaunch baxter_kinect_calibration baxter_bundle_calibrate_xtion.launch

view onlive results: 

rosrun tf tf_echo /torso /camera_link

once we have the matrix transformation from tf tf_echo, we copy the translation and rotation part and publish a static transformation:

e.g.:

matrix from tf_echo:

Translation: [0.760, 0.799, 0.907]
- Rotation: in Quaternion [0.336, 0.351, -0.588, 0.647]
            in RPY (radian) [0.043, 1.014, -1.450]
            in RPY (degree) [2.492, 58.080, -83.103]

static transformation:

rosrun tf static_transform_publisher 1.267 0.14 0.3 -0.355 0.014 0.935 -0.000 /world /camera_link 100

there is an offset in the translation, you can modify (by hand) this values until you get the desired accuracy:

*rosrun tf stic_transform_publisher 0.810, 0.809, 0.747 0.336, 0.351, -0.588, 0.647 /world /camera_link 10

)))))))))

#### launch affordance example:

First, launch the camera node (se above)

Second, launch the perception node to extract coordinate points of the objects of intereste (in Omar's code is called poi):

rosrun pr2_ransac_supervoxels pr2_ransac_supervoxels_node

Third, launch the experiment manager (kind of menu that choses an object and action to perform on) to control the perception, action execution and effect detection.

roslaunch baxter_experiment_manager experiment_launcher_innorobot.launch


#### Baxter arm modes:

http://sdk.rethinkrobotics.com/wiki/Arm_Control_Modes

The suggested one is the joint control mode.

to see an example see Omar's baxter_basic_action node, scripts: baxter_basic_actions_ik_moveit, joint_position_basic_actions and remember to copy ik_solver.py because this one performs the IK, cartersian to joint space.



/// carlos camera link
1.343 0.16 0.3 -0.390 -0.033 0.919 -0.041
- Rotation: in Quaternion [-0.390, -0.033, 0.919, -0.041]
            in RPY (radian) [-0.042, 0.804, -3.070]
            in RPY (degree) [-2.403, 46.064, -175.888]

/// carlos camera rbg link
1.099 0.203 0.414 0.611 0.726 -0.231 -0.214
- Rotation: in Quaternion [0.611, 0.726, -0.231, -0.214]
            in RPY (radian) [-2.500, -0.028, 1.732]
            in RPY (degree) [-143.265, -1.619, 99.237]


/// our calibration stuff for camera infront of the table with regard to /camera_link frame
1.148 0.14 0.3 -0.423 -0.051 0.902 -0.067
- Rotation: in Quaternion [-0.423, -0.051, 0.902, -0.067]
            in RPY (radian) [-0.054, 0.879, -3.019]
            in RPY (degree) [-3.094, 50.370, -172.951]


/// our calibration stuff for camera infront of the table with regard to /camera_rgb_optical_frame
1.267, 0.185, 0.299 0.652, 0.638, -0.297, -0.283
- Translation: [1.267, 0.185, 0.299]
- Rotation: in Quaternion [0.652, 0.638, -0.297, -0.283]
            in RPY (radian) [-2.297, 0.026, 1.561]
            in RPY (degree) [-131.589, 1.500, 89.431]

/// our calibration stuff for camera infront of the table with regard to /camera_depth_optical_frame
1.267 0.160 0.299 0.652 0.638 -0.297 -0.283
- Translation: [1.267, 0.160, 0.299]
- Rotation: in Quaternion [0.652, 0.638, -0.297, -0.283]
            in RPY (radian) [-2.297, 0.026, 1.561]
            in RPY (degree) [-131.589, 1.500, 89.431]



/// our calibration stuff for camera behind the robot
-0.55, 0.18, 0.68 -0.206 0.259 0.540 0.774
- Rotation: in Quaternion [-0.206, 0.259, 0.540, 0.774]
            in RPY (radian) [-0.051, 0.672, 1.201]
            in RPY (degree) [-2.905, 38.521, 68.807]

//helpful tip to launch moveit with kinect camera
roslaunch baxter_moveit_config demo_baxter.launch right_electric_gripper:="true" left_electric_gripper:="true" kinect:="true" camera_link_pose:="1.21 0.2 0.33 -0.438 -0.031 0.898 -0.016"

