How to use the Baxter
=====================

Setup commands

kinect

####  launch kinect:

roslaunch freenect_launch freenect.launch rgb_frame_id:=camera_rgb_optical_frame depth_frame_id:=camera_depth_optical_frame


((((((
if camera not calibrated:

publish calibration based on QR code as reference point (both the external (camera_link) and arm camera (base_link) must watch the QR codes):

source /home/chavez/catkin_ws/devel/setup.bash

roslaunch baxter_kinect_calibration baxter_bundle_calibrate_xtion.launch

view onlive results: 

rosrun tf tf_echo /base /camera_link

once we have the matrix transformation from tf tf_echo, we copy the translation and rotation part and publish a static transformation:

crustcrawler transformation matrix:
 -0.700171    -0.1149  -0.704669      0.898
  0.144045  -0.989404  0.0182026     -0.225
 -0.699293 -0.0887589   0.709303      0.813
         0          0          0          1



e.g.:

Translation: [0.760, 0.799, 0.907]
- Rotation: in Quaternion [0.336, 0.351, -0.588, 0.647]
            in RPY (radian) [0.043, 1.014, -1.450]
            in RPY (degree) [2.492, 58.080, -83.103]

Stop previous runs.

Run static transformation:

rosrun tf static_transform_publisher 1.134 0.14 0.46 -0.504 -0.091 0.853 -0.097 /base /camera_link 100

there is an offset in the translation, you can modify (by hand) this values until you get the desired accuracy

now, we need the transformation with the camera_rgb_optical_frame for the internal CPP matrix (both the external and arm camera must watch the QR codes):

rosrun tf tf_echo /base /camera_rbg_optical_frame


)))))))))

#### launch affordance example:

First, launch the camera node (se above)

Second, launch the perception node to extract coordinate points of the objects of intereste (in Omar's code is called poi):

rosrun pr2_ransac_supervoxels pr2_ransac_supervoxels_node

Third, launch the experiment manager (kind of menu that choses an object and action to perform on) to control the perception, action execution and effect detection.

roslaunch baxter_experiment_manager experiment_launcher_innorobot.launch


#### Baxter arm modes:

http://sdk.rethinkrobotics.com/wiki/Arm_Control_Modes

The suggested one is the joint control mode.

to see an example see Omar's baxter_basic_action node, scripts: baxter_basic_actions_ik_moveit, joint_position_basic_actions and remember to copy ik_solver.py because this one performs the IK, cartersian to joint space.



/// carlos /camera_link frame
1.134 0.14 0.46 -0.504 -0.091 0.853 -0.097
- Rotation: in Quaternion [-0.504, -0.091, 0.853, -0.097]
            in RPY (radian) [-0.119, 1.072, -2.987]
            in RPY (degree) [-6.842, 61.441, -171.123]


/// carlos /camera_rgb_optical_frame
- Translation: [1.338, 0.205, 0.301]
- Rotation: in Quaternion [0.618, 0.692, -0.269, -0.261]
            in RPY (radian) [-2.375, -0.029, 1.672]
            in RPY (degree) [-136.074, -1.644, 95.808]


/// our calibration stuff for camera infront of the table with regard to /camera_link frame
1.148 0.14 0.3 -0.423 -0.051 0.902 -0.067
- Rotation: in Quaternion [-0.423, -0.051, 0.902, -0.067]
            in RPY (radian) [-0.054, 0.879, -3.019]
            in RPY (degree) [-3.094, 50.370, -172.951]


/// our calibration stuff for camera infront of the table with regard to /camera_rgb_optical_frame
1.267, 0.185, 0.299 0.652, 0.638, -0.297, -0.283
- Translation: [1.267, 0.185, 0.299]
- Rotation: in Quaternion [0.652, 0.638, -0.297, -0.283]
            in RPY (radian) [-2.297, 0.026, 1.561]
            in RPY (degree) [-131.589, 1.500, 89.431]

/// our calibration stuff for camera infront of the table with regard to /camera_depth_optical_frame
1.267 0.160 0.299 0.652 0.638 -0.297 -0.283
- Translation: [1.267, 0.160, 0.299]
- Rotation: in Quaternion [0.652, 0.638, -0.297, -0.283]
            in RPY (radian) [-2.297, 0.026, 1.561]
            in RPY (degree) [-131.589, 1.500, 89.431]



/// our calibration stuff for camera behind the robot
-0.55, 0.18, 0.68 -0.206 0.259 0.540 0.774
- Rotation: in Quaternion [-0.206, 0.259, 0.540, 0.774]
            in RPY (radian) [-0.051, 0.672, 1.201]
            in RPY (degree) [-2.905, 38.521, 68.807]

//helpful tip to launch moveit with kinect camera
roslaunch baxter_moveit_config demo_baxter.launch right_electric_gripper:="true" left_electric_gripper:="true" kinect:="true" camera_link_pose:="1.21 0.2 0.33 -0.438 -0.031 0.898 -0.016"

