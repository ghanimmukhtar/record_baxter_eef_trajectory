How to use the Baxter
=====================

Setup commands

kinect

####  launch kinect:

roslaunch freenect_launch freenect.launch rgb_frame_id:=camera_rgb_optical_frame depth_frame_id:=camera_depth_optical_frame


((((((
if camera not calibrated:

launch calibration

roslaunch baxter_kinect_calibration baxter_bundle_calibrate_xtion.launch

view onlive results: 

rosrun tf tf_echo /torso /camera_link

once we have the matrix transformation from tf tf_echo, we copy the translation and rotation part and publish a static transformation:

e.g.:

matrix from tf_echo:

Translation: [0.760, 0.799, 0.907]
- Rotation: in Quaternion [0.336, 0.351, -0.588, 0.647]
            in RPY (radian) [0.043, 1.014, -1.450]
            in RPY (degree) [2.492, 58.080, -83.103]

static transformation:

rosrun tf static_transform_publisher 0.760, 0.799, 0.907 0.336, 0.351, -0.588, 0.647 /world /camera_link 10

there is an offset in the translation, you can modify (by hand) this values until you get the desired accuracy:

*rosrun tf stic_transform_publisher 0.810, 0.809, 0.747 0.336, 0.351, -0.588, 0.647 /world /camera_link 10

)))))))))

#### launch affordance example:

First, launch the camera node (se above)

Second, launch the perception node to extract coordinate points of the objects of intereste (in Omar's code is called poi):

rosrun pr2_ransac_supervoxels pr2_ransac_supervoxels_node

Third, launch the experiment manager (kind of menu that choses an object and action to perform on) to control the perception, action execution and effect detection.

roslaunch baxter_experiment_manager experiment_launcher_innorobot.launch


#### Baxter arm modes:

http://sdk.rethinkrobotics.com/wiki/Arm_Control_Modes

The suggested one is the joint control mode.

to see an example see Omar's baxter_basic_action node, scripts: baxter_basic_actions_ik_moveit, joint_position_basic_actions and remember to copy ik_solver.py because this one performs the IK, cartersian to joint space.


/// our calibration stuff for camera infront of the table
1.21 0.2 0.33 -0.438 -0.031 0.898 -0.016 
- Rotation: in Quaternion [-0.438 -0.031 0.898 -0.016 ]
            in RPY (radian) [-0.066, 0.907, -3.137]
            in RPY (degree) [-3.782, 51.983, -179.747]

rotation matrix:
[ INFO] [1486646729.780078930]: element in row: 0 column: 0 is: -0.615594
[ INFO] [1486646729.780155203]: element in row: 0 column: 1 is: 0.0559219
[ INFO] [1486646729.780178521]: element in row: 0 column: 2 is: -0.786077
[ INFO] [1486646729.780197333]: element in row: 1 column: 0 is: -0.00158085
[ INFO] [1486646729.780215311]: element in row: 1 column: 1 is: -0.997565
[ INFO] [1486646729.780233169]: element in row: 1 column: 2 is: -0.0697293
[ INFO] [1486646729.780250111]: element in row: 2 column: 0 is: -0.788062
[ INFO] [1486646729.780267894]: element in row: 2 column: 1 is: -0.0416823
[ INFO] [1486646729.780284912]: element in row: 2 column: 2 is: 0.614184



/// our calibration stuff for camera behind the robot
-0.55, 0.18, 0.68 -0.206 0.259 0.540 0.774
- Rotation: in Quaternion [-0.206, 0.259, 0.540, 0.774]
            in RPY (radian) [-0.051, 0.672, 1.201]
            in RPY (degree) [-2.905, 38.521, 68.807]

//helpful tip to launch moveit with kinect camera
roslaunch baxter_moveit_config demo_baxter.launch right_electric_gripper:="true" left_electric_gripper:="true" kinect:="true" camera_link_pose:="1.21 0.2 0.33 -0.438 -0.031 0.898 -0.016"

